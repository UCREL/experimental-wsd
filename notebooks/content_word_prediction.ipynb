{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26806239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wn\n",
    "from wn.compat import sensekey\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from experimental_wsd.config import RaganatoEnglish, DATA_PROCESSING_DIR\n",
    "from experimental_wsd.wsd import wsd_sentence_generator\n",
    "from experimental_wsd.wordnet_utils import check_lexicon_exists\n",
    "from experimental_wsd.training_utils import get_prefix_suffix_special_token_indexes\n",
    "from experimental_wsd.training_utils import write_to_jsonl\n",
    "\n",
    "semcor_data_directory = RaganatoEnglish.semcor\n",
    "\n",
    "EN_LEXICON = 'omw-en:1.4'\n",
    "check_lexicon_exists(EN_LEXICON)\n",
    "ENGLISH_WN = wn.Wordnet(lexicon=EN_LEXICON, expand='')\n",
    "GET_SENSE = sensekey.sense_getter(EN_LEXICON, ENGLISH_WN)\n",
    "GET_SENSE_ID = sensekey.sense_key_getter(EN_LEXICON)\n",
    "\n",
    "for example in wsd_sentence_generator(semcor_data_directory, GET_SENSE):\n",
    "    annotation = example.annotations\n",
    "    print(example.annotations)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WN.sense(\"omw-en-sign-14301785-n\").synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78556d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_SENSE(\"try%2:41:00::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_SENSE(\"try%2:41:00::\").synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_SENSE_ID(wn.sense(\"omw-en-good-01123148-a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.id for a in ENGLISH_WN.senses(\"at last\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcff726",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in annotation:\n",
    "    print(item)\n",
    "    print(item.labels)\n",
    "    print(GET_SENSE(item.labels[0]).synset().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aaa378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec71a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "a_name = \"jhu-clsp/ettin-encoder-17m\"\n",
    "a_name = \"FacebookAI/roberta-base\"\n",
    "a_model = AutoModel.from_pretrained(a_name)\n",
    "a_config = AutoConfig.from_pretrained(a_name)\n",
    "a_tokenizer = AutoTokenizer.from_pretrained(a_name, add_prefix=True)\n",
    "a_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WN.sense(\"omw-en-New_York-09118181-n\").synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \"omw-en-be-02702508-v\",\n",
    "        ],\n",
    "        [\"omw-en-New_York-09119277-n\", \"omw-en-New_York-09118181-n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WN.sense(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84088d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WN.senses(\"be\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c3495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb209a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WN.words(\"new york\")[0].senses()[0].synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tokenizer.decode([0,20920,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42674672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\", add_prefix_space=True\n",
    ")\n",
    "sentence = [\"Hello how are you\", \"I am ok\", \"\"]\n",
    "tokenized_sentence = TOKENIZER(sentence)\n",
    "for key, value in tokenized_sentence.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde35503",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Hello\", \"how\", \"are\", \"you\"]\n",
    "tokenized_sentence = TOKENIZER(sentence, is_split_into_words=True)\n",
    "for key, value in tokenized_sentence.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence.word_ids(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Hello how are you\", \"a Test\"]\n",
    "tokenized_sentence = a_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Hello how are you\", \"a Test\"]\n",
    "tokenized_sentence = a_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "states = a_model(**tokenized_sentence, output_hidden_states=True).hidden_states\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import inspect\n",
    "inspect.getfullargspec(transformers.RobertaModel.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3750a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wn\n",
    "from wn.compat import sensekey\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from experimental_wsd.config import RaganatoEnglish, DATA_PROCESSING_DIR\n",
    "from experimental_wsd.wsd import wsd_sentence_generator\n",
    "from experimental_wsd.wordnet_utils import check_lexicon_exists\n",
    "from experimental_wsd.training_utils import get_prefix_suffix_special_token_indexes\n",
    "from experimental_wsd.training_utils import write_to_jsonl\n",
    "from experimental_wsd.data_processing_utils import map_token_text_and_is_content_labels, tokenize_and_align_labels\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
    "alt_tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/ettin-encoder-17m\", add_prefix_space=True)\n",
    "\n",
    "\n",
    "semcor_data_directory = RaganatoEnglish.semcor\n",
    "\n",
    "EN_LEXICON = 'omw-en:1.4'\n",
    "check_lexicon_exists(EN_LEXICON)\n",
    "ENGLISH_WN = wn.Wordnet(lexicon=EN_LEXICON, expand='')\n",
    "GET_SENSE = sensekey.sense_getter(EN_LEXICON, ENGLISH_WN)\n",
    "\n",
    "DATA_PROCESSING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "semcor_jsonl_file_path = write_to_jsonl(wsd_sentence_generator(semcor_data_directory, GET_SENSE), DATA_PROCESSING_DIR, 'semcor', overwrite=False)\n",
    "semcor_dataset = load_dataset(\"json\", data_files=str(semcor_jsonl_file_path))\n",
    "semcor_dataset_splits = semcor_dataset['train'].train_test_split(0.1)\n",
    "\n",
    "is_content_word_dataset = semcor_dataset_splits.map(map_token_text_and_is_content_labels, remove_columns=semcor_dataset_splits['train'].column_names, batched=False)\n",
    "labelled_content_word_dataset = is_content_word_dataset.map(tokenize_and_align_labels, batched=True, fn_kwargs={\"tokenizer\": tokenizer}, remove_columns=is_content_word_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTextEncoding, AutoModel\n",
    "\n",
    "\n",
    "model = AutoModelForTextEncoding.from_pretrained(\"FacebookAI/roberta-base\", add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental_wsd.nn.token_classifier import TokenClassifier\n",
    "\n",
    "test_classifier = TokenClassifier(\"FacebookAI/roberta-base\", True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from experimental_wsd.training_utils import AscendingSequenceLengthBatchSampler, collate_token_classification_dataset\n",
    "\n",
    "_collate_function = collate_token_classification_dataset(tokenizer)\n",
    "train_dataset = labelled_content_word_dataset['train']\n",
    "train_batch_sampler = AscendingSequenceLengthBatchSampler(train_dataset, batch_size=4, length_key='input_ids', random=True)\n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, collate_fn=_collate_function, batch_sampler=train_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d55782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "output = None\n",
    "for sample in train_dataset_loader:\n",
    "    test_classifier(sample['input_ids'], sample['attention_mask'], sample['word_ids'])\n",
    "    break\n",
    "    #print(sample['input_ids'].shape)\n",
    "    #input = [train_dataset[index] for index in sample]\n",
    "    #print(input)\n",
    "    #print(_collate_function(input))\n",
    "    #print(len(sample['input_ids']))\n",
    "    #break\n",
    "print(time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef889026",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a48ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36452b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(output.hidden_states[1].shape)\n",
    "torch.sum(torch.stack(output.hidden_states, dim=1), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = labelled_content_word_dataset['train'].take(11)\n",
    "sampler = AccedingSequenceLengthBatchSampler(small_dataset, 4, 'input_ids', random=True, with_replacement=False)\n",
    "for batch in sampler:\n",
    "    print(batch)\n",
    "    batch = batch_content_word_dataset(alt_tokenizer)([small_dataset[index] for index in batch])\n",
    "    print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c89c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600380c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3,4], dtype=torch.long)\n",
    "b = torch.tensor([2,2,3,2], dtype=torch.long)\n",
    "torch.hstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2e17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a007f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_dataset:\n",
    "    print(sample['input_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f98f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_content_word_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daad7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hf_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dcc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hf_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(hf_gen()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_wsl_sentence_to_hf_generator(wsl_generator):\n",
    "    def _hf_generator():\n",
    "        for sentence in wsl_generator:\n",
    "            yield {\"1\": \"2\"}\n",
    "    return _hf_generator\n",
    "\n",
    "hf_gen = generic_wsl_sentence_to_hf_generator(wsd_sentence_generator(semcor_data_directory, GET_SENSE))\n",
    "IterableDataset.from_generator(hf_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
    "alt_tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/ettin-encoder-17m\", add_prefix_space=True)\n",
    "\n",
    "prefix_suffix_tokens = get_prefix_suffix_special_token_indexes(tokenizer)\n",
    "alt_prefix_suffix_tokens = get_prefix_suffix_special_token_indexes(alt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59161f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = [token.raw for token in sentence_data.tokens]\n",
    "content_labels = [int(token.is_content_word) for token in sentence_data.tokens]\n",
    "alt_tokenizer.decode(alt_tokenizer(sentence_tokens, is_split_into_words=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_tokenizer(sentence_tokens, is_split_into_words=True).word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_labels_with_tokens([0,1,0], alt_tokenizer(sentence_tokens, is_split_into_words=True).word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc5746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_tokenizer.decode(alt_tokenizer(\"hello\")[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = alt_tokenizer([\"Hello how are\", \"Hi\"], padding=True, truncation=True, add_special_tokens=False)\n",
    "tokens[1].special_tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[1].ids[0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f023fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_tokenizer.decode(get_prefix_suffix_special_token_indexes(alt_tokenizer)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ab6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(tokenizer, PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_special_tokens, suffix_special_tokens = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix_suffix_special_token_indexes(a_tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast) -> tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Finds the prefix and suffix special token ids for a given tokenizer when the \n",
    "    tokenizer tokenizes text (this does not include padding or attention mask), e.g. \n",
    "    for Roberta this would be `([0], [2])` whereby these ids represent the \n",
    "    token strings `([<s>], [</s>])`\n",
    "\n",
    "    These token ids are a list as some tokenizers may have 0 to many prefix and \n",
    "    suffix special tokens.\n",
    "\n",
    "    Args:\n",
    "        a_tokenizer (PreTrainedTokenizer | PreTrainedTokenizerFast): The \n",
    "            tokenizer to get the special token ids for.\n",
    "    Returns:\n",
    "        tuple[list[int], list[int]]: The prefix and suffix special token ids.\n",
    "    \"\"\"\n",
    "    batch_encoding = tokenizer(\"a\")\n",
    "    sentence_encoding = batch_encoding[0]\n",
    "    special_token_mask = sentence_encoding.special_tokens_mask\n",
    "    prefix_ids: list[int] = []\n",
    "    suffix_ids: list[int] = []\n",
    "    token_ids = sentence_encoding.ids\n",
    "    is_prefix = True\n",
    "    for token_index, token_index_value in enumerate(special_token_mask):\n",
    "        if token_index_value == 0:\n",
    "            is_prefix = False\n",
    "            continue\n",
    "        if is_prefix:\n",
    "            prefix_ids.append(token_ids[token_index])\n",
    "        else:\n",
    "            suffix_ids.append(token_ids[token_index])\n",
    "    return prefix_ids, suffix_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a50f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_data in wsd_sentence_generator(semcor_data_directory, GET_SENSE):\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36536b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceda6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162761ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline[0][1].model.dim_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do with spacy for testing is to download the roberta model from huggingface and add it to spacy\n",
    "# To run spacy in GPU mode we will need to change the base image of the Nvidia dockerfile to \n",
    "# one that contains nvcc which I think is the dev base image.\n",
    "test_sentences = [\"Hello how are you\", \"I'am well and yourself?\"]\n",
    "tokvecs = []\n",
    "vectors = None\n",
    "for doc in nlp.pipe(test_sentences):\n",
    "    print(doc._.trf_data.all_outputs[0].data[0])\n",
    "\t#vectors = doc._.trf_data.tensors[-1]\n",
    "    #tokvecs.append(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f29634",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(test_sentences[0], return_tensors='pt')\n",
    "model.eval()\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "\toutput = model(**encoded_input, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.hidden_states[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_SENSE(\"editorial%1:10:00::\").synset().relations()\n",
    "GET_SENSE(\"enterprise%1:04:00::\").synset().definition()\n",
    "ENGLISH_WN.words(\"enterprise\", pos=\"n\")[0].synsets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimental-wsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
